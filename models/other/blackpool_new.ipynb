{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "from multi_scale_unet import UNET\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as tvtransforms\n",
    "import time\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "import metrics as smpmetrics\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "from meter import AverageValueMeter\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTICLASS_MODE: str = \"multiclass\"\n",
    "ENCODER = \"resnet18\"\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['background', 'ocean', 'wetsand', 'buildings', 'vegetation', 'drysand']\n",
    "ACTIVATION = None\n",
    "BATCH_SIZE = 1\n",
    "MODEL_NAME = 'unet'\n",
    "#MIN_HEIGHT = 256\n",
    "#MIN_WIDTH = 256\n",
    "MIN_HEIGHT = 2048\n",
    "MIN_WIDTH = 256\n",
    "EPOCHS = 100\n",
    "DEVICE = 'cuda'\n",
    "TRAIN_IMG_DIR = \"../../CoastSat/data/blackpool/images/train/\"\n",
    "TRAIN_MASK_DIR = \"../../CoastSat/data/blackpool/images/trainannot/\"\n",
    "TEST_IMG_DIR = \"../../CoastSat/data/blackpool/images/test/\"\n",
    "TEST_MASK_DIR = \"../../CoastSat/data/blackpool/images/testannot/\"\n",
    "VAL_IMG_DIR = '../../CoastSat/data/blackpool/images/val/'\n",
    "VAL_MASK_DIR = '../../CoastSat/data/blackpool/images/valannot/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation image date: 2017-01-05-11-24-39\n",
    "Training image date: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avail = torch.cuda.is_available() # just checking which devices are available for training\n",
    "devCnt = torch.cuda.device_count()\n",
    "devName = torch.cuda.get_device_name(0)\n",
    "print(\"Available: \" + str(avail) + \", Count: \" + str(devCnt) + \", Name: \" + str(devName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    \"\"\"This method creates the dataset from given directories\"\"\"\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        \"\"\"initialize directories\n",
    "\n",
    "        :image_dir: TODO\n",
    "        :mask_dir: TODO\n",
    "        :transform: TODO\n",
    "\n",
    "        \"\"\"\n",
    "        self._image_dir = image_dir\n",
    "        self._mask_dir = mask_dir\n",
    "        self._transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "        self.mapping = {(0, 0, 0): 0, # background class (black)\n",
    "                        (0, 0, 255): 1,  # 0 = class 1\n",
    "                        (225, 0, 225): 2,  # 1 = class 2\n",
    "                        (255, 0, 0): 3,  # 2 = class 3\n",
    "                        (255, 225, 225): 4, # 3 = class 4\n",
    "                        (255, 255, 0): 5}  # 4 = class 5\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"returns length of images\n",
    "        :returns: TODO\n",
    "\n",
    "        \"\"\"\n",
    "        return len(self.images)\n",
    "    \n",
    "    def mask_to_class_rgb(self, mask):\n",
    "        #print('----mask->rgb----')\n",
    "        h=20\n",
    "        w=722\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = torch.squeeze(mask)  # remove 1\n",
    "\n",
    "        # check the present values in the mask, 0 and 255 in my case\n",
    "        #print('unique values rgb    ', torch.unique(mask)) \n",
    "        # -> unique values rgb     tensor([  0, 255], dtype=torch.uint8)\n",
    "\n",
    "        class_mask = mask\n",
    "        class_mask = class_mask.permute(2, 0, 1).contiguous()\n",
    "        h, w = class_mask.shape[1], class_mask.shape[2]\n",
    "        mask_out = torch.zeros(h, w, dtype=torch.long)\n",
    "\n",
    "        for k in self.mapping:\n",
    "            idx = (class_mask == torch.tensor(k, dtype=torch.uint8).unsqueeze(1).unsqueeze(2))         \n",
    "            validx = (idx.sum(0) == 3)          \n",
    "            mask_out[validx] = torch.tensor(self.mapping[k], dtype=torch.long)\n",
    "\n",
    "        # check the present values after mapping, in my case 0, 1, 2, 3PSPNet\n",
    "        #print('unique values mapped ', torch.unique(mask_out))\n",
    "        # -> unique values mapped  tensor([0, 1, 2, 3])\n",
    "       \n",
    "        return mask_out\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"TODO: Docstring for __getitem__.\n",
    "        :returns: TODO\n",
    "\n",
    "        \"\"\"\n",
    "        img_path = os.path.join(self._image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self._mask_dir, self.images[index])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"RGB\"))\n",
    "        mask = self.mask_to_class_rgb(mask).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "        if self._transform is not None:\n",
    "            augmentations = self._transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "            \n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(\n",
    "    train_dir,\n",
    "    train_mask_dir,\n",
    "    val_dir,\n",
    "    val_mask_dir,\n",
    "    batch_size,\n",
    "    train_transform,\n",
    "    val_transform,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    This method creates the dataloader objects for the training loops\n",
    "\n",
    "    :train_dir: directory of training images\n",
    "    :train_mask_dir: directory of training masks\n",
    "    :val_dir: validation image directory\n",
    "\n",
    "    :returns: training and validation dataloaders\n",
    "    recall\n",
    "    \"\"\"\n",
    "    \n",
    "    train_ds = Dataset(image_dir=train_dir,\n",
    "                             mask_dir=train_mask_dir,\n",
    "                             transform=train_transform)\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_ds = Dataset(\n",
    "        image_dir=val_dir,\n",
    "        mask_dir=val_mask_dir,\n",
    "        transform=val_transform,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Training and testing image transforms using albumentation libraries\n",
    "    \"\"\"\n",
    "test_transform = A.Compose(\n",
    "    [A.PadIfNeeded(min_height=MIN_HEIGHT, min_width=MIN_WIDTH, border_mode=4),A.Resize(MIN_HEIGHT, MIN_WIDTH),]\n",
    ")\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.PadIfNeeded(min_height=MIN_HEIGHT, min_width=MIN_WIDTH, border_mode=4),\n",
    "        A.Resize(MIN_HEIGHT, MIN_WIDTH),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.MedianBlur(blur_limit=3, always_apply=False, p=0.1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataloaders\n",
    "trainDL, testDL = get_loaders(TRAIN_IMG_DIR, TRAIN_MASK_DIR,\n",
    "                                           TEST_IMG_DIR, TEST_MASK_DIR,\n",
    "                                           BATCH_SIZE, train_transform,\n",
    "                                           test_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize(**images):\n",
    "    \"\"\"Method to generate visualization of the training images\n",
    "    \"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16,5))\n",
    "\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_delete = A.Compose([\n",
    "        A.PadIfNeeded(32,512),\n",
    "        A.Resize(32, 512),\n",
    "        ToTensorV2(),\n",
    "    ], )\n",
    "dataset = Dataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, transform=transforms_delete)\n",
    "\n",
    "image, mask = dataset[10] # get some sample\n",
    "\n",
    "#visualize(\n",
    "#    image=image,\n",
    "#    mask=mask.squeeze()\n",
    "#)\n",
    "#mask = mask.unsqueeze(0)\n",
    "\n",
    "print(mask.shape)\n",
    "print(image.shape)\n",
    "unique, counts = np.unique(mask, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "print(image.shape, image.dtype, type(image), mask.shape, \n",
    "mask.dtype, type(mask), mask.min(), \n",
    "image.max(), mask.min(), mask.max())\n",
    "\n",
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(mask)\n",
    "plt.show()\n",
    "\n",
    "#mask = mask.unsqueeze(0)\n",
    "#print(mask.shape)\n",
    "\n",
    "print(image.shape, image.dtype, type(image), mask.shape, \n",
    "mask.dtype, type(mask), mask.min(), \n",
    "image.max(), mask.min(), mask.max())\n",
    "\n",
    "print(len(CLASSES))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "#model = smp.Unet(\n",
    "#    encoder_name=ENCODER, \n",
    "#    encoder_weights=ENCODER_WEIGHTS, \n",
    "#    in_channels=3,\n",
    "#    classes=len(CLASSES),\n",
    "#    activation=ACTIVATION,\n",
    "#)\n",
    "\n",
    "wavelet_model = UNET(in_channels=3, out_channels=6).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "loss = smp.losses.DiceLoss(mode=\"multiclass\")\n",
    "loss.__name__ = 'Dice_loss'\n",
    "\n",
    "# metrics have been defined in the custom training loop as giving them in a list object did not work for me\n",
    "metrics = [\n",
    "\n",
    "]\n",
    "\n",
    "# define optimizer and learning rate\n",
    "optimizer = optim.Adam(params=wavelet_model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(metrics, loader, model, device='cpu'):\n",
    "    \"\"\" Custom method to calculate accuracy of testing data\n",
    "    :loader: dataloader objects\n",
    "    :model: model to test\n",
    "    :device: cpu or gpu\n",
    "    \"\"\"\n",
    "\n",
    "    # define scores to track\n",
    "    f1_score = 0\n",
    "    precision_score = 0\n",
    "    recall_score = 0\n",
    "    iou_score = 0\n",
    "    dataset_size = len(loader.dataset)  # number of images in the dataloader\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    model.eval() # set model for evaluation\n",
    "    with torch.no_grad():  # do not calculate gradients\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).unsqueeze(1)\n",
    "            y = y.to(torch.int64)\n",
    "            #x = x.permute(0,1,2,3) # ===========================================================================\n",
    "            x = x.permute(0,2,3,1)\n",
    "            preds = model(x.float().contiguous())  # get pixel predictions from image tensor\n",
    "            preds = torch.argmax(preds, dim=1).unsqueeze(1).int()  # get maximum values of tensor along dimension 1\n",
    "\n",
    "\n",
    "            #print(preds.shape, y.shape)\n",
    "            tp, fp, fn, tn = smpmetrics.get_stats(preds, y, mode='multiclass', num_classes=6)  # get tp,fp,fn,tn from predictions\n",
    "\n",
    "            # compute metric\n",
    "            a = smpmetrics.iou_score(tp, fp, fn, tn, reduction=\"macro\")\n",
    "            b = smpmetrics.f1_score(tp,fp,fn,tn, reduction='macro')\n",
    "            c = smpmetrics.precision(tp,fp,fn,tn, reduction='macro')\n",
    "            d = smpmetrics.recall(tp,fp,fn,tn, reduction='macro')\n",
    "\n",
    "\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(y)\n",
    "\n",
    "            #confmat = ConfusionMatrix(num_classes=6)\n",
    "            #print(preds.shape, y.shape)\n",
    "            #print(confmat(preds.cpu(), y.cpu()))\n",
    "\n",
    "\n",
    "            iou_score += a\n",
    "            f1_score += b\n",
    "            precision_score += c\n",
    "            recall_score += d\n",
    "\n",
    "    iou_score /= dataset_size  # averaged score across all images in directory\n",
    "    f1_score /= dataset_size\n",
    "    precision_score /= dataset_size\n",
    "    recall_score /= dataset_size\n",
    "    #print(type(y_pred[0]), y_pred[0].shape, y_true[0].shape, len(y_pred), len(y_true), )\n",
    "\n",
    "    #xut = y_pred[0]\n",
    "    #xutrue=y_true[0]\n",
    "\n",
    "    #ax = plt.axes()\n",
    "\n",
    "    #confmat = ConfusionMatrix(num_classes=6, normalize='true')\n",
    "    #df_cm = confmat(xut.cpu(), xutrue.cpu())\n",
    "    #df_cm = pd.DataFrame(df_cm.numpy())\n",
    "\n",
    "    #sn.set(font_scale=0.9)\n",
    "    #sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 12}) # font size\n",
    "\n",
    "    #ax.set_title('{} Confusion Matrix'.format(MODEL_NAME))\n",
    "    #ax.set_xticklabels(CLASSES, rotation=40)\n",
    "    #ax.set_yticklabels(CLASSES, rotation=40)\n",
    "    #plt.savefig('./{}_heatmap.png'.format(MODEL_NAME),dpi=300, bbox_inches = \"tight\")\n",
    "    #plt.show()\n",
    "\n",
    "    #print(df_cm)\n",
    "    #print(xut.shape)\n",
    "\n",
    "    #plt.close()    \n",
    "    print('IOU Score: {} | F1 Score: {} | Precision Score: {} | Recall Score: {}'.format(iou_score, f1_score, precision_score, recall_score))\n",
    "    #model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def train_wavelet(loader, model, optimizer, loss_fn, scaler):\n",
    "    \"\"\"TODO: Docstring for train_fn.\n",
    "\n",
    "    :loader: TODO\n",
    "    :model: TODO\n",
    "    :optimizer: TODO\n",
    "    :loss_fn: TODO\n",
    "    :scaler: TODO\n",
    "    :returns: TODO\n",
    "\n",
    "    \"\"\"\n",
    "    loop = tqdm(loader)\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE).float()\n",
    "        targets = targets.to(device=DEVICE)\n",
    "        targets = targets.long()\n",
    "        data = data.permute(0,3,1,2)  # correct shape for image\n",
    "        targets = targets.squeeze(1)\n",
    "\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data.contiguous())\n",
    "            loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # update loop\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        del loss, predictions\n",
    "\n",
    "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
    "    \"\"\" Custom training loop for models\n",
    "\n",
    "    :loader: dataloader object\n",
    "    :model: model to train\n",
    "    :optimizer: training optimizer\n",
    "    :loss_fn: loss function\n",
    "    :scaler: scaler object\n",
    "    :returns:\n",
    "\n",
    "    \"\"\"\n",
    "    loop = tqdm(loader)  # just a nice library to keep track of loops\n",
    "    model = model.to(DEVICE)# ===========================================================================\n",
    "    for batch_idx, (data, targets) in enumerate(loop):  # iterate through dataset\n",
    "        data = data.to(device=DEVICE).float()\n",
    "        targets = targets.to(device=DEVICE).float()\n",
    "        targets = targets.unsqueeze(1)\n",
    "        data = data.permute(0,3,2,1)  # correct shape for image# ===========================================================================\n",
    "        targets = targets.to(torch.int64)\n",
    "\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # loss_values.append(loss.item())\n",
    "        #run['training/batch/loss'].log(loss)\n",
    "\n",
    "        #update loop\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "for epoch in range(EPOCHS):  # run training and accuracy functions and save model\n",
    "    #train_fn(trainDL, model, optimizer, loss, scaler)\n",
    "    train_wavelet(trainDL, wavelet_model, optimizer, loss, scaler)\n",
    "    check_accuracy(metrics, testDL, wavelet_model, DEVICE)\n",
    "    torch.save(wavelet_model, './{}.pth'.format(MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using saved model\n",
    "\n",
    "val_transform = A.Compose(  # validation image transforms\n",
    "    [A.Resize(MIN_HEIGHT, MIN_WIDTH),ToTensorV2()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = torch.load('./saved_models/{}/{}.pth'.format(MODEL_NAME,MODEL_NAME))  # load model\n",
    "#model_eval = torch.load('./{}.pth'.format(MODEL_NAME))  # load model\n",
    "print('{} loaded'.format(MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stack all tiled validation images together and save image as predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module\n",
    "import os\n",
    "import torchmetrics\n",
    "metric_f1 = torchmetrics.F1(average='macro', num_classes=6, mdmc_average='global', multiclass=True)\n",
    "confmat = ConfusionMatrix(num_classes=6, normalize='true')\n",
    "# assign directory\n",
    "#directory = '../../CoastSat/data/blackpool/images/val2/'\n",
    "directory = '../../CoastSat/data/blackpool/images/val/'\n",
    "#directory = '../bp_validation_images/'\n",
    "image_matrix = np.zeros(shape=(10,256))\n",
    "f1_eval = 0\n",
    "precision_eval = 0\n",
    "recall_eval = 0\n",
    "iou_eval = 0\n",
    "colors = [(0, 0, 255/255), (225/255, 0, 225/255), (255/255, 0, 0), (255/255, 225/255, 225/255), (255/255, 255/255, 0)]\n",
    "value_names = ['ocean', 'wetsand', 'buildings', 'vegetation', 'drysand']\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over files in\n",
    "# that directory\n",
    "model_eval.eval() # set model for evaluation\n",
    "with torch.no_grad():  # do not calculate gradients\n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        f = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "            val_image = Image.open(f)\n",
    "            val_mask = Image.open('/home/jont/Documents/CoastSat/data/blackpool/images/valannot/bp_test.jpg')\n",
    "            \n",
    "            transformed = val_transform(image=np.array(val_image))\n",
    "            image_transformed = transformed['image']\n",
    "\n",
    "            batch_tensor = torch.unsqueeze(image_transformed, 0)\n",
    "            out = model_eval(batch_tensor.to(device=DEVICE).float())\n",
    "\n",
    "            preds = torch.argmax(out, dim=1)\n",
    "            preds = preds.cpu().detach().permute(1,2,0)\n",
    "            \n",
    "            m_transformed = val_transform(image=np.array(val_mask))\n",
    "            mask_transformed = m_transformed['image']\n",
    "            m_batch_tensor = torch.unsqueeze(mask_transformed,0)\n",
    "            m_gt = torch.argmax(m_batch_tensor, dim=1)\n",
    "            m_gt = m_gt.permute(1,2,0)\n",
    "\n",
    "            preds = preds[:,:,0]\n",
    "            m_gt = m_gt[:,:,0]\n",
    "            \n",
    "            print(metric_f1(preds, m_gt))\n",
    "            # ax = plt.axes()\n",
    "\n",
    "            # df_cm = confmat(preds.cpu(), m_gt.cpu())\n",
    "            # df_cm = pd.DataFrame(df_cm.numpy())\n",
    "\n",
    "            # sn.set(font_scale=0.9)\n",
    "            # sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 12}) # font size\n",
    "\n",
    "            # ax.set_title('{} Confusion Matrix'.format(MODEL_NAME))\n",
    "            # ax.set_xticklabels(CLASSES, rotation=40)\n",
    "            # ax.set_yticklabels(CLASSES, rotation=40)\n",
    "            # plt.savefig('./{}_heatmap.png'.format(MODEL_NAME),dpi=300, bbox_inches = \"tight\")\n",
    "            # plt.show()\n",
    "        \n",
    "            preds_np = preds.numpy()\n",
    "\n",
    "            image_matrix = np.vstack((image_matrix, preds_np))\n",
    "\n",
    "image_matrix = image_matrix[10:,:]\n",
    "\n",
    "plt.imshow(image_matrix)\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "plt.title(\"{} Predictions\".format(MODEL_NAME))\n",
    "#plt.grid(True)\n",
    "#plt.savefig('blackpool_preds_{}.png'.format(MODEL_NAME))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "matplotlib.image.imsave('bp_full_{}.png'.format(MODEL_NAME), image_matrix)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./bp_full_wavelet-unet.png')\n",
    "res = cv2.resize(img, dsize=(256, 2048), interpolation=cv2.INTER_CUBIC)\n",
    "#cv2.imshow('image',res)\n",
    "cv2.imwrite('./bp__val_full_wavelet_final.png', res)\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation accuracy results\n",
    "trainDL_old, valDL = get_loaders(TRAIN_IMG_DIR, TRAIN_MASK_DIR,\n",
    "                                           VAL_IMG_DIR, VAL_MASK_DIR,\n",
    "                                           BATCH_SIZE, val_transform,\n",
    "                                           val_transform)\n",
    "\n",
    "check_accuracy(metrics, valDL, model_eval,DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "path = \"./saved_images/full_images/\"\n",
    "valid_images = [\".jpg\",\".gif\",\".png\",\".tga\"]\n",
    "for f in os.listdir(path):\n",
    "    ext = os.path.splitext(f)[1]\n",
    "    if ext.lower() not in valid_images:\n",
    "        continue\n",
    "    img = np.array(Image.open(os.path.join(path,f)))\n",
    "    imgs.append(img)\n",
    "    print(f)\n",
    "\n",
    "myorder = [5, 1, 2, 0, 6, 3, 4, 7]\n",
    "imgs = [imgs[i] for i in myorder]\n",
    "print(len(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_images(images, cols = 1, titles = None):\n",
    "    \n",
    "    \"\"\"Display a list of images in a single figure with matplotlib.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "    cols (Default = 1): Number of columns in figure (number of rows is \n",
    "                        set to np.ceil(n_images/float(cols))).\n",
    "    \n",
    "    titles: List of titles corresponding to each image. Must have\n",
    "            the same length as images.\n",
    "    \"\"\"\n",
    "    assert((titles is None)or (len(images) == len(titles)))\n",
    "    n_images = len(images)\n",
    "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "    fig = plt.figure(facecolor=\"#e1ddbf\")\n",
    "    for n, (image, title) in enumerate(zip(images, titles)):\n",
    "        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n",
    "        if image.ndim == 2:\n",
    "            plt.gray()\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title, fontdict={'fontsize': 20, 'fontweight': 'medium'})\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
    "    plt.savefig('./all_model_results.png', facecolor=fig.get_facecolor())\n",
    "    plt.show()\n",
    "titles=['Ground Truth RGB Image', 'Ground Truth Mask', 'Wavelet U-Net', 'U-net', 'U-Net++', 'PSPNet', 'MaNet', 'FPN']\n",
    "show_images(images=imgs, cols=1, titles=titles)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a7cb2997f409818a2f560ca2b7e2dddd818ab4252eb1067f63da94cd1a1f870"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('lambda-stack-with-tensorflow-pytorch': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
