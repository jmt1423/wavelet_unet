{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as tvtransforms\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import metrics as smpmetrics\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "from meter import AverageValueMeter\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "import segmentation_models_pytorch as smp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTICLASS_MODE: str = \"multiclass\"\n",
    "ENCODER = \"resnet18\"\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['background', 'ocean', 'wetsand', 'buildings', 'vegetation', 'drysand']\n",
    "ACTIVATION = None\n",
    "BATCH_SIZE = 2\n",
    "MODEL_NAME = 'deeplabv3'\n",
    "EPOCHS = 200\n",
    "DEVICE = 'cuda'\n",
    "TRAIN_IMG_DIR = \"../../CoastSat/data/blackpool/images/train/\"\n",
    "TRAIN_MASK_DIR = \"../../CoastSat/data/blackpool/images/trainannot/\"\n",
    "VAL_IMG_DIR = \"../../CoastSat/data/blackpool/images/test/\"\n",
    "VAL_MASK_DIR = \"../../CoastSat/data/blackpool/images/testannot/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avail = torch.cuda.is_available() # just checking which devices are available for training\n",
    "devCnt = torch.cuda.device_count()\n",
    "devName = torch.cuda.get_device_name(0)\n",
    "print(\"Available: \" + str(avail) + \", Count: \" + str(devCnt) + \", Name: \" + str(devName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    \"\"\"This method creates the dataset from given directories\"\"\"\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        \"\"\"initialize directories\n",
    "\n",
    "        :image_dir: TODO\n",
    "        :mask_dir: TODO\n",
    "        :transform: TODO\n",
    "\n",
    "        \"\"\"\n",
    "        self._image_dir = image_dir\n",
    "        self._mask_dir = mask_dir\n",
    "        self._transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "        self.mapping = {(0, 0, 0): 0, # background class (black)\n",
    "                        (0, 0, 255): 1,  # 0 = class 1\n",
    "                        (225, 0, 225): 2,  # 1 = class 2\n",
    "                        (255, 0, 0): 3,  # 2 = class 3\n",
    "                        (255, 225, 225): 4, # 3 = class 4\n",
    "                        (255, 255, 0): 5}  # 4 = class 5\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"returns length of images\n",
    "        :returns: TODO\n",
    "\n",
    "        \"\"\"\n",
    "        return len(self.images)\n",
    "    \n",
    "    def mask_to_class_rgb(self, mask):\n",
    "        #print('----mask->rgb----')\n",
    "        h=20\n",
    "        w=722\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = torch.squeeze(mask)  # remove 1\n",
    "\n",
    "        # check the present values in the mask, 0 and 255 in my case\n",
    "        #print('unique values rgb    ', torch.unique(mask)) \n",
    "        # -> unique values rgb     tensor([  0, 255], dtype=torch.uint8)\n",
    "\n",
    "        class_mask = mask\n",
    "        class_mask = class_mask.permute(2, 0, 1).contiguous()\n",
    "        h, w = class_mask.shape[1], class_mask.shape[2]\n",
    "        mask_out = torch.zeros(h, w, dtype=torch.long)\n",
    "\n",
    "        for k in self.mapping:\n",
    "            idx = (class_mask == torch.tensor(k, dtype=torch.uint8).unsqueeze(1).unsqueeze(2))         \n",
    "            validx = (idx.sum(0) == 3)          \n",
    "            mask_out[validx] = torch.tensor(self.mapping[k], dtype=torch.long)\n",
    "\n",
    "        # check the present values after mapping, in my case 0, 1, 2, 3PSPNet\n",
    "        #print('unique values mapped ', torch.unique(mask_out))\n",
    "        # -> unique values mapped  tensor([0, 1, 2, 3])\n",
    "       \n",
    "        return mask_out\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"TODO: Docstring for __getitem__.\n",
    "        :returns: TODO\n",
    "\n",
    "        \"\"\"\n",
    "        img_path = os.path.join(self._image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self._mask_dir, self.images[index])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"RGB\"))\n",
    "        mask = self.mask_to_class_rgb(mask).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "        if self._transform is not None:\n",
    "            augmentations = self._transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "            \n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(\n",
    "    train_dir,\n",
    "    train_mask_dir,\n",
    "    val_dir,\n",
    "    val_mask_dir,\n",
    "    batch_size,\n",
    "    train_transform,\n",
    "    val_transform,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    This method creates the dataloader objects for the training loops\n",
    "\n",
    "    :train_dir: directory of training images\n",
    "    :train_mask_dir: directory of training masks\n",
    "    :val_dir: validation image directory\n",
    "\n",
    "    :returns: training and validation dataloaders\n",
    "recall\n",
    "    \"\"\"\n",
    "    train_ds = Dataset(image_dir=train_dir,\n",
    "                             mask_dir=train_mask_dir,\n",
    "                             transform=train_transform)\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_ds = Dataset(\n",
    "        image_dir=val_dir,\n",
    "        mask_dir=val_mask_dir,\n",
    "        transform=val_transform,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Training and testing image transforms using albumentation libraries\n",
    "    \"\"\"\n",
    "test_transform = A.Compose(\n",
    "    [A.PadIfNeeded(min_height=32, min_width=512, border_mode=4),A.Resize(32, 512),]\n",
    ")\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.PadIfNeeded(min_height=32, min_width=512, border_mode=4),\n",
    "        A.Resize(32, 512),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.MedianBlur(blur_limit=3, always_apply=False, p=0.1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataloaders\n",
    "trainDL, testDL = get_loaders(TRAIN_IMG_DIR, TRAIN_MASK_DIR,\n",
    "                                           VAL_IMG_DIR, VAL_MASK_DIR,\n",
    "                                           BATCH_SIZE, train_transform,\n",
    "                                           test_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def visualize(**images):\n",
    "    \"\"\"Method to generate visualization of the training images\n",
    "    \"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16,5))\n",
    "\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_delete = A.Compose([\n",
    "        A.PadIfNeeded(32,512),\n",
    "        A.Resize(32, 512),\n",
    "        ToTensorV2(),\n",
    "    ], )\n",
    "dataset = Dataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, transform=transforms_delete)\n",
    "\n",
    "image, mask = dataset[10] # get some sample\n",
    "\n",
    "#visualize(\n",
    "#    image=image,\n",
    "#    mask=mask.squeeze()\n",
    "#)\n",
    "#mask = mask.unsqueeze(0)\n",
    "\n",
    "print(mask.shape)\n",
    "print(image.shape)\n",
    "unique, counts = np.unique(mask, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "print(image.shape, image.dtype, type(image), mask.shape, \n",
    "mask.dtype, type(mask), mask.min(), \n",
    "image.max(), mask.min(), mask.max())\n",
    "\n",
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(mask)\n",
    "plt.show()\n",
    "\n",
    "#mask = mask.unsqueeze(0)\n",
    "#print(mask.shape)\n",
    "\n",
    "print(image.shape, image.dtype, type(image), mask.shape, \n",
    "mask.dtype, type(mask), mask.min(), \n",
    "image.max(), mask.min(), mask.max())\n",
    "\n",
    "print(len(CLASSES))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "model = smp.DeepLabV3(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    in_channels=3,\n",
    "    classes=len(CLASSES),\n",
    "    activation=ACTIVATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "loss = smp.losses.DiceLoss(mode=\"multiclass\")\n",
    "loss.__name__ = 'Dice_loss'\n",
    "\n",
    "# metrics have been defined in the custom training loop as giving them in a list object did not work for me\n",
    "metrics = [\n",
    "\n",
    "]\n",
    "\n",
    "# define optimizer and learning rate\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training epoch\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss,\n",
    "    metrics= metrics,\n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# testing epoch\n",
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(metrics, loader, model, device='cpu'):\n",
    "    \"\"\" Custom method to calculate accuracy of testing data\n",
    "    :loader: dataloader objects\n",
    "    :model: model to test\n",
    "    :device: cpu or gpu\n",
    "    \"\"\"\n",
    "\n",
    "    # define scores to track\n",
    "    f1_score = 0\n",
    "    precision_score = 0\n",
    "    recall_score = 0\n",
    "    iou_score = 0\n",
    "    dataset_size = len(testDL.dataset)  # number of images in the dataloader\n",
    "\n",
    "    model.eval() # set model for evaluation\n",
    "    with torch.no_grad():  # do not calculate gradients\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).unsqueeze(1)\n",
    "            y = y.to(torch.int64)\n",
    "            x = x.permute(0,3,1,2)\n",
    "            preds = model(x.float())  # get pixel predictions from image tensor\n",
    "            preds = torch.argmax(preds, dim=1).unsqueeze(1).int()  # get maximum values of tensor along dimension 1\n",
    "\n",
    "            tp, fp, fn, tn = smpmetrics.get_stats(preds, y, mode='multiclass', num_classes=6)  # get tp,fp,fn,tn from predictions\n",
    "\n",
    "            # compute metric\n",
    "            a = smpmetrics.iou_score(tp, fp, fn, tn, reduction=\"macro\")\n",
    "            b = smpmetrics.f1_score(tp,fp,fn,tn, reduction='macro')\n",
    "            c = smpmetrics.precision(tp,fp,fn,tn, reduction='macro')\n",
    "            d = smpmetrics.recall(tp,fp,fn,tn, reduction='macro')\n",
    "\n",
    "            iou_score += a\n",
    "            f1_score += b\n",
    "            precision_score += c\n",
    "            recall_score += d\n",
    "\n",
    "    iou_score /= dataset_size  # averaged score across all images in directory\n",
    "    f1_score /= dataset_size\n",
    "    precision_score /= dataset_size\n",
    "    recall_score /= dataset_size\n",
    "\n",
    "    \n",
    "    print('IOU Score: {} | F1 Score: {} | Precision Score: {} | Recall Score: {}'.format(iou_score, f1_score, precision_score, recall_score))\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
    "    \"\"\" Custom training loop for models\n",
    "\n",
    "    :loader: dataloader object\n",
    "    :model: model to train\n",
    "    :optimizer: training optimizer\n",
    "    :loss_fn: loss function\n",
    "    :scaler: scaler object\n",
    "    :returns:\n",
    "\n",
    "    \"\"\"\n",
    "    loop = tqdm(loader)  # just a nice library to keep track of loops\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loop):  # iterate through dataset\n",
    "        data = data.to(device=DEVICE).float() \n",
    "        targets = targets.to(device=DEVICE)\n",
    "        targets = targets.unsqueeze(1)\n",
    "        data = data.permute(0,3,1,2)  # correct shape for image\n",
    "        targets = targets.to(torch.int64)\n",
    "\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # loss_values.append(loss.item())\n",
    "        #run['training/batch/loss'].log(loss)\n",
    "\n",
    "        #update loop\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "for epoch in range(EPOCHS):  # run training and accuracy functions and save model\n",
    "    train_fn(trainDL, model, optimizer, loss, scaler)\n",
    "    check_accuracy(metrics, testDL, model, DEVICE)\n",
    "    torch.save(model, './{}.pth'.format(MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using saved model\n",
    "\n",
    "val_transform = A.Compose(  # validation image transforms\n",
    "    [A.Resize(32, 512),ToTensorV2()]\n",
    ")\n",
    "\n",
    "val_image = Image.open('../../CoastSat/data/blackpool/images/val/tile_0-2960.png')\n",
    "\n",
    "#val_image.show()\n",
    "\n",
    "transformed = val_transform(image=np.array(val_image))\n",
    "image_transformed = transformed['image']\n",
    "batch_tensor = torch.unsqueeze(image_transformed, 0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./{}.pth'.format(MODEL_NAME))  # load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stack all tiled validation images together and save image as predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module\n",
    "import os\n",
    "# assign directory\n",
    "directory = '../../CoastSat/data/blackpool/images/val/'\n",
    "image_matrix = np.zeros(shape=(32,512))\n",
    "\n",
    "\n",
    "# iterate over files in\n",
    "# that directory\n",
    "for filename in sorted(os.listdir(directory)):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        val_image = Image.open(f)\n",
    "        transformed = val_transform(image=np.array(val_image))\n",
    "        image_transformed = transformed['image']\n",
    "        batch_tensor = torch.unsqueeze(image_transformed, 0)\n",
    "        out = model(batch_tensor.to(device=DEVICE).float())\n",
    "\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "        preds = preds.cpu().detach().permute(1,2,0)\n",
    "        preds = preds[:,:,0]\n",
    "\n",
    "        preds_np = preds.numpy()\n",
    "\n",
    "        image_matrix = np.vstack((image_matrix, preds_np))\n",
    "\n",
    "image_matrix = image_matrix[32:,:]\n",
    "plt.imshow(image_matrix)\n",
    "plt.savefig('blackpool_preds_{}.png'.format(MODEL_NAME))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for the rgb image. \n",
    "\n",
    "Image size is 32,512,3 - so you have to divide the final tensor object by 255 for matplotlib to plot the rgb original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../../CoastSat/data/blackpool/images/val/'\n",
    "image_matrix1 = np.zeros(shape=(32,512,3))\n",
    "#print(image_matrix.shape)\n",
    "#testmat = np.vstack((image_matrix, preds.numpy()))\n",
    "\n",
    " \n",
    "# iterate over files in\n",
    "# that directory\n",
    "for filename in sorted(os.listdir(directory)):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        val_image = Image.open(f)\n",
    "        transformed = val_transform(image=np.array(val_image))\n",
    "        image_transformed = transformed['image']\n",
    "        img3 = image_transformed.permute(1,2,0)\n",
    "        #img3 = img3[:,:,0]\n",
    "        print(img3.shape)\n",
    "        image_matrix1 = np.vstack((image_matrix1,img3))\n",
    "\n",
    "image_matrix1 = image_matrix1[32:,:]/255\n",
    "plt.imshow(image_matrix1)\n",
    "plt.savefig('blackpool_gt_rgb_{}.png'.format(MODEL_NAME))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, get the ground truth masks basically same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import os\n",
    "# assign directory\n",
    "directory3 = '../../CoastSat/data/blackpool/images/valannot/'\n",
    "image_matrix3 = np.zeros(shape=(32,512,3))\n",
    "\n",
    "\n",
    "# iterate over files in\n",
    "# that directory\n",
    "for filename in sorted(os.listdir(directory3)):\n",
    "    f = os.path.join(directory3, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        val_image = Image.open(f)\n",
    "        transformed = val_transform(image=np.array(val_image))\n",
    "        image_transformed = transformed['image']\n",
    "        img3 = image_transformed.permute(1,2,0)\n",
    "        image_matrix3 = np.vstack((image_matrix3, img3))\n",
    "\n",
    "\n",
    "image_matrix3 = image_matrix3[32:,:]\n",
    "plt.imshow(image_matrix3)\n",
    "plt.savefig('blackpool_gt_mask_{}.png'.format(MODEL_NAME))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a7cb2997f409818a2f560ca2b7e2dddd818ab4252eb1067f63da94cd1a1f870"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('lambda-stack-with-tensorflow-pytorch': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
